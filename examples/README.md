# Examples

Each of the codes can be easily launched using python: 

                                                      python code_name.py
                                                      
However, each code also accepts some options allowing to test more in detail the different blocks of our proposed method.

## I) Feature Extraction

Feature extraction using an auto-encoder can be done by launching the code *feature_extraction.py*. There are no particular arguments in this code and the training parameters of the auto-encoder are the same of the ones indicated in the submitted paper.

The results of this code will be saved in *models/MNIST_EXP_ID/Projections/ where *EXP* is the name in the parameters file *parameters_file/default_parameters_AE.json* and *ID* is an identifier to identify several exepriments with the same name. The results are:
* ../models/MNIST_EXP_ID/Model/model.pth: A pth file containing the parameters of the trained autoencoder.
* ../models/MNIST_EXP_ID/Model/metrics.hdf5: An hdf5 file containing the metrics of the training and testing of the model
* ../models/MNIST_EXP_ID/CompressedRepresentations/training_representations.pth: A pth file containing different information from the different compressed training points. When loaded into python it is a list where each element corresponds to a dict representing the points over the different epochs. The keys of each element (which are dicts) are:
    * 'compressed_representation': Compressed Representation (in terms of dimensionality) of the original data sample.
    * 'label': Class (or scores) of the reduced dim sample
*  ../models/MNIST_EXP_ID/CompressedRepresentations/testing_representations_MNIST_ID.txt: Same as before but for the testing data.

## II) Dimensionality Reduction and Optimal Projection Selection

The code *dim_reduction.py* allows to test the second step of our method (Dimensionality Reduction and Optimal Projection Selection). The code will start compute several 2D projections of the embedded representations obtained by an auto-encoder (this step can take a considerable amount of time, to reduce this time, we recommend to modify the t-SNE grid search parameters in the code *src/tsne_grid_search.py*). Then, the code will determine the best, middle and worst projections according to our selection strategy based on the *Silhouette Score*. No particular arguments are needed in this code.

The results of this code will be saved in *models/MNIST_Example_0/*. The results are various folders stored in ../models/MNIST_EXP_ID/Projections/ named EmbeddedRepresentations_perpVal_lrVal_earlyExVal_dimVal. Each folder contains five files:
- labels_ID.pth: List of labels of the different 2D/3D points generated by t-SNE.
- originalImages_ID.pth: List of images representing the different 2D/3D
points generated by TSNE.
- representations_ID.pth: List of the 2D/3D points generated by t-SNE.

The code will also generate a file ../models/MNIST_Example_0/Projections/resultsProjectionSelection.json containing the best, middle and worst projections with their respectives Silhouettes Scores.

## III) Label propagation

The final step of our semi-automatic annotation method is done throught the code *label_propagation.py*. This code can be executed without any argument but it can also be launched with several arguments:
* **exp_ID**: Name of the experiment.
* **folder_embRepr**: Folder to the files describing the embedded data.
* **propagation_mode**: Mode to propagate labels (propLocalQual, classicalProp or OPF-Semi).
* **var_to_study**: Variable to study (K, percentageLabelsKeep or localQualThresh).
* **sorted_qualities**: True if wanted to sort the samples by local quality when propagating the labels using LQ-KNN.
* **local_quality_threshold**: Local quality threshold to use if propLocalQual mode is used and the variable to study is not the local quality threshold.
* **ks**: Value of ks to choose the local quality file to use for LQ-kNN.
* **kt**: Value of kt to choose the local quality file to use for LQ-kNN.

It stores the results in a folder named 'LabelPropResults' in the same folder as the embedded representations (i.e. in folder_embRepr). The name of the file is of the form expID_propMode-{}_var-to-study-{}.pth

 ## IV) Classification with a semi-automatically labeled dataset
 
 The classification performances using a semi-automatically labeled dataset can be obtained using the code *label_propagation_with_classification.py*. This codes admits two arguments:
 * **parameters_file**: Path to a parameters file containing the different parameters of the experiment. The default used file is parameters_files/default_parameters_classification*.
 * **folder_embRepr**: Path to the folder containing the final embedded representations that are going to be used to do label propagation and create the final training set. This folder is obtained by using *examples/dim_reduction.py* or *src/tsne_grid_search.py*. 

The code generates a file with the metrics of the experiment (no model is saved as several models are trained based on the number of repetitions chosen). This file is stored in folder_embRepr/ClassificationResults/ (if the ClassificationResults folder does not exists, it is created by the code).
 
 **WARNING**: Two of the parameters of the parameters file are the values of *k_s* and *k_t*. This means that in the folder containing the embedded representations (i.e. the folder that is going to be as argument in *folder_embRepr*), the local qualities for the chosen values of *k_s* and *k_t* should be computed (this can be done by using the code *src/projection_metrics.py*.
 
 ## V) Proposed method: from beginning to end
 
The code *full_pipeline.py* can be used to test our proposed method from beginning (i.e. feature extraction) to end (i.e. label propagation). This code admits four arguments:
* **propagation_mode**: Mode to propagate labels (propLocalQual, classicalProp or OPF-Semi)
* **local_quality_threshold**: Local quality threshold to use if propLocalQual mode is used and the variable to study is not the local quality threshold
* **sorted_qualities**: True if wanted to sort the samples by local quality when propagating the labels using LQ-KNN
* **var_to_study**: Variable to study (K, percentageLabelsKeep or localQualThresh)

The folders where the results are stored are of the same form as in the previous examples. All the paths to these folders are printed by the code during execution so we recommend to save the output of the code in a log file.

